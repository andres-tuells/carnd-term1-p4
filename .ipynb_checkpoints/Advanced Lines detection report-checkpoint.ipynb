{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Advanced lines detection\n",
    "\n",
    "The objective of this project is to use advanced techniques of computer vision to detect the lanes on the road.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.image as mpimage\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import pickle\n",
    "import PIL.Image\n",
    "import IPython.display\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "# Imports to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "\n",
    "The first step is to calibrate the camera. All cameras deformates the image in some way or another. We use a set of checkboard images to measure the distortion and calirate the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_images_from_dir(dirname):\n",
    "    '''\n",
    "    loads the images in directory dirname and transforms to RGB format\n",
    "    '''\n",
    "    rs = [cv2.imread(\"{}/{}\".format(dirname, image)) for image in os.listdir(dirname)]\n",
    "    return [cv2.cvtColor(r, cv2.COLOR_BGR2RGB) for r in rs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set of calibration images\n",
    "calibration_images = load_images_from_dir(\"./camera_cal\")\n",
    "\n",
    "\n",
    "def show_images(images, figsize, nrows=5, ncols=5, cmap=None):\n",
    "    '''\n",
    "    show a set of images in a grid\n",
    "    '''\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    for image, ax in zip(images,axes.flat):\n",
    "        ax.imshow(image, cmap=cmap)\n",
    "        \n",
    "#shows the set of calibration images\n",
    "show_images(calibration_images, figsize=(20,10), nrows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test images\n",
    "test_images = load_images_from_dir(\"./test_images\")\n",
    "\n",
    "#show the set of test images\n",
    "show_images(test_images, figsize=(20,10), nrows=2, ncols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring distortion\n",
    "\n",
    "We measure the distortion using cv2 function findChessboardCorners. We use this functions to deect the checkboard corners and the distortion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calibrate_images(images, nx=9,ny=6):\n",
    "    '''\n",
    "    calibrate a set of checkboard images. \n",
    "    nx is the number of corners in x direction.\n",
    "    ny is the number of corners in y direction.\n",
    "    '''\n",
    "    # termination criteria\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "    objp = np.zeros((nx*ny,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:nx,0:ny].T.reshape(-1, 2)\n",
    "\n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d point in real world space\n",
    "    imgpoints = [] # 2d points in image plane.\n",
    "    rs_images = [] # result images\n",
    "\n",
    "    for img in images:\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Find the chess board corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (nx,ny),None)\n",
    "\n",
    "        # If found, add object points, image points (after refining them)\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "\n",
    "            corners2 = cv2.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria)\n",
    "            imgpoints.append(corners2)\n",
    "\n",
    "            # Draw and display the corners\n",
    "            img = cv2.drawChessboardCorners(img, (nx,ny), corners2,ret)\n",
    "            rs_images.append(img)\n",
    "    return objpoints, imgpoints, rs_images\n",
    "\n",
    "objpoints, imgpoints, calibrated_images = calibrate_images(calibration_images)\n",
    "\n",
    "#we show the checkboard images already calibrated.\n",
    "show_images(calibrated_images, figsize=(20,10), nrows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#parameters of camera calibration\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, calibration_images[0].shape[1::-1],None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def undistord_image(image):\n",
    "    '''\n",
    "    using the previous parameters of camera calibration returns an undistorted version of the image.\n",
    "    '''\n",
    "    return cv2.undistort(image, mtx, dist, None, mtx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_undistorted_images(original_images):\n",
    "    undistorted_images = [undistord_image(image) for image in original_images]\n",
    "    fig, axes = plt.subplots(len(original_images), 2, figsize=(10, 20))\n",
    "    #fig.subplots_adjust(hspace=4, wspace=6)\n",
    "    images = list(zip(original_images, undistorted_images))\n",
    "    j=0\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        if i%2==0:\n",
    "            ax.imshow(original_images[j])\n",
    "            ax.set_xlabel('Original image')\n",
    "        else:\n",
    "            ax.imshow(undistorted_images[j])\n",
    "            ax.set_xlabel('Undistorted image')\n",
    "            j +=1\n",
    "        ax.set_xticks([], [])\n",
    "        ax.set_yticks([], [])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()\n",
    "    \n",
    "#show test images and undistorted version\n",
    "show_undistorted_images(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show project video\n",
    "video_input = 'project_video.mp4'\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(video_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pipelined_video(pipeline, video_output, video_input='project_video.mp4'):\n",
    "    '''\n",
    "    helper video that process input video through pipeline and writes to video_output\n",
    "    '''\n",
    "    clip1 = VideoFileClip(video_input)\n",
    "    white_clip = clip1.fl_image(pipeline) #NOTE: this function expects color images!!\n",
    "    %time white_clip.write_videofile(video_output, audio=False)  \n",
    "    return HTML(\"\"\"\n",
    "    <video width=\"960\" height=\"540\" controls>\n",
    "      <source src=\"{0}\">\n",
    "    </video>\n",
    "    \"\"\".format(video_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show the project video undistorted\n",
    "pipelined_video(undistord_image, 'undist_project_video.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective transform\n",
    "\n",
    "The camera is in the front of the car. We want to change the perspective to a perspective from above, so it's is more easy to detect the lane lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "undistorted_test_images = [undistord_image(image) for image in test_images]\n",
    "image = undistorted_test_images[2]\n",
    "\n",
    "#show an undistorted image from the test images set.\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def region_of_interest(img, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, [vertices], ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "# Define the region\n",
    "area_of_interest = np.array([[150+430,460],[1150-440,460],[1150,720],[150,720]], dtype=np.int32)\n",
    "masked_image = region_of_interest(image, area_of_interest)\n",
    "\n",
    "#show the region of interest to process the image\n",
    "imshow(masked_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have selected visualy an area of interest. We use the selected area of interest to warp later the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_shape = image.shape\n",
    "img_size = (image_shape[1], image_shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def warp_image(image, rect=[[150+430,460],[1150-440,460],[1150,720],[150,720]]):\n",
    "    '''\n",
    "    warp the image to change perspective.\n",
    "    the rect area is the same as area of interest before.\n",
    "    we add an offset to include more area around our region of interest\n",
    "    '''\n",
    "    # construct our destination points which will be used to\n",
    "    # map the screen to a top-down, \"birds eye\" view\n",
    "    offset1 = 200 # offset for dst points x value\n",
    "    offset2 = 0 # offset for dst points bottom y value\n",
    "    offset3 = 0 # offset for dst points top y value\n",
    "    # Grab the image shape\n",
    "    img_size = (image.shape[1], image.shape[0])\n",
    "    # For destination points, I'm arbitrarily choosing some points to be\n",
    "    # a nice fit for displaying our warped result \n",
    "    dst = np.float32([[offset1, offset3], \n",
    "                      [img_size[0]-offset1, offset3], \n",
    "                      [img_size[0]-offset1, img_size[1]-offset2], \n",
    "                      [offset1, img_size[1]-offset2]])\n",
    "    \n",
    "    src = np.float32(rect)\n",
    " \n",
    "    # calculate the perspective transform matrix and warp\n",
    "    # the perspective to grab the screen\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    warp = cv2.warpPerspective(image, M, img_size)\n",
    "    return warp, M, Minv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show a warped image.Now we have a perspective from above\n",
    "imshow(warp_image(image)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pipeline_warped(image):\n",
    "    '''\n",
    "    this pipeline includes undistord image and warp image\n",
    "    '''\n",
    "    image = undistord_image(image)\n",
    "    image = warp_image(image)[0]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show the warped video\n",
    "pipelined_video(pipeline_warped, 'warped_project_video.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sharpen image\n",
    "def sharpen_image(img, s=1.0):\n",
    "    gb = cv2.GaussianBlur(img, (5,5), 20.0)\n",
    "    img = cv2.addWeighted(img, 2, gb, -1, 0)\n",
    "    return img\n",
    "\n",
    "# Compute linear image transformation img*s+m\n",
    "def lin_img(img,s=1.0,m=0.0):\n",
    "    img2=cv2.multiply(img, np.array([s]))\n",
    "    return cv2.add(img2, np.array([m]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pipeline_sharped(image):\n",
    "    '''\n",
    "    we add a gaussian blur and sharpen image to improve image quality for computer vision\n",
    "    '''\n",
    "    image = undistord_image(image)\n",
    "    image = warp_image(image)[0]\n",
    "    image = sharpen_image(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show the sharped video\n",
    "pipelined_video(pipeline_sharped, 'sharped_project_video.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color mask\n",
    "\n",
    "To help us to find the lanes lines we filter the image. In this section we apply the color filter. We filter the white and yellow colors. \n",
    "\n",
    "For the white filter we use and rgb filter box between gray (200,200,200) and white (255,255,255)\n",
    "\n",
    "For the yellow filter we transform the image to hsv and appy a minimum and maximum threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mask_white_or_yellow(image):\n",
    "    \"\"\"\n",
    "    Filter the image to include only yellow and white pixels\n",
    "    \"\"\"\n",
    "    # Filter white pixels\n",
    "    white_threshold = 200 #130\n",
    "    lower_white = np.array([white_threshold, white_threshold, white_threshold])\n",
    "    upper_white = np.array([255, 255, 255])\n",
    "    white_mask = cv2.inRange(image, lower_white, upper_white)\n",
    "    white_image = cv2.bitwise_and(image, image, mask=white_mask)\n",
    "\n",
    "    # Filter yellow pixels\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    #parameters obtained manualy. The previous conversion was from BGR image that was encoded as RGB\n",
    "    lower_yellow = np.array([90,100,100])\n",
    "    upper_yellow = np.array([110,255,255])\n",
    "    yellow_mask = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "    yellow_image = cv2.bitwise_and(image, image, mask=yellow_mask)\n",
    "    \n",
    "    return cv2.bitwise_or(white_image, yellow_image)\n",
    "\n",
    "    # Combine the two above images\n",
    "    image2 = cv2.addWeighted(white_image, 1., yellow_image, 1., 0.)\n",
    "\n",
    "    return image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show image filtered by color white and yellow\n",
    "imshow(mask_white_or_yellow(image), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pipeline_color(image):\n",
    "    '''\n",
    "    in this pipeline we include the color filter\n",
    "    '''\n",
    "    image = undistord_image(image)\n",
    "    image = warp_image(image)[0]\n",
    "    image = sharpen_image(image)\n",
    "    image = mask_white_or_yellow(image)\n",
    "    return image\n",
    "\n",
    "#show the video after applying the color filter\n",
    "pipelined_video(pipeline_color, 'color_project_video.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient threshold\n",
    "\n",
    "At the same time of the color filter we apply the gradient filter. Applying the sobel function to the x axis we detect the gradient changes. We use that to detect the lane lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show image in grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "imshow(gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "# Read in an image and grayscale it\n",
    "#image = mpimg.imread('signs_vehicles_xygrad.png')\n",
    "\n",
    "# Define a function that applies Sobel x or y, \n",
    "# then takes an absolute value and applies a threshold.\n",
    "# Note: calling your function with orient='x', thresh_min=5, thresh_max=100\n",
    "# should produce output like the example image shown above this quiz.\n",
    "# Define a function that takes an image, gradient orientation,\n",
    "# and threshold min / max values.\n",
    "def abs_sobel_thresh(img, orient='x', thresh_min=0, thresh_max=255):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS).astype(np.float)\n",
    "    #h_channel = hls[:,:,0]\n",
    "    gray = hls[:,:,1]\n",
    "    # Apply x or y gradient with the OpenCV Sobel() function\n",
    "    # and take the absolute value\n",
    "    if orient == 'x':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0))\n",
    "    if orient == 'y':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1))\n",
    "    # Rescale back to 8 bit integer\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    # Create a copy and apply the threshold\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    # Here I'm using inclusive (>=, <=) thresholds, but exclusive is ok too\n",
    "    binary_output[(scaled_sobel >= thresh_min) & (scaled_sobel <= thresh_max)] = 1\n",
    "\n",
    "    # Return the result\n",
    "    return binary_output\n",
    "\n",
    "\n",
    "def sobel_transform(image):\n",
    "    \"\"\"\n",
    "    apply the sobel thresh function with the desired threshold.\n",
    "    \"\"\"\n",
    "    return abs_sobel_thresh(image, orient='x', thresh_min=20, thresh_max=255)\n",
    "    #return abs_sobel_thresh(image, orient='x', thresh_min=20, thresh_max=100)\n",
    "    \n",
    "# Run the function\n",
    "grad_binary = sobel_transform(image)\n",
    "# Plot the result\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(image)\n",
    "ax1.set_title('Original Image', fontsize=50)\n",
    "ax2.imshow(grad_binary, cmap='gray')\n",
    "ax2.set_title('Thresholded Gradient', fontsize=50)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mask_gradient_and_color(image):\n",
    "    \"\"\"\n",
    "    apply gradient and color mask\n",
    "    \"\"\"\n",
    "    color = mask_white_or_yellow(image)\n",
    "    grad = sobel_transform(image)\n",
    "    grad_and_color = cv2.bitwise_and(color, color, mask=grad)\n",
    "    return grad_and_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show test images through the gradient and color mask.\n",
    "show_images( [mask_gradient_and_color(image) for image in test_images], figsize=(20,10), nrows=2, ncols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pipeline_grad_and_color(image):\n",
    "    \"\"\"\n",
    "    include in the pipeline the gradient and color mask\n",
    "    \"\"\"\n",
    "    image = undistord_image(image)\n",
    "    image = warp_image(image)[0]\n",
    "    image = sharpen_image(image)\n",
    "    image = mask_gradient_and_color(image)\n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show the test images through the gradient and color mask.\n",
    "#we check visually the presence of points to be able to draw the lines.\n",
    "show_images( [pipeline_grad_and_color(image) for image in test_images], figsize=(20,10), nrows=2, ncols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show the video through the gradient and color pipeline\n",
    "#we check visualy that we don't lose the lines in the video. We adjust the parameters until we don't lose them.\n",
    "pipelined_video(pipeline_grad_and_color, 'grad_and_color_project_video.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding lines\n",
    "\n",
    "After we have filtered the images to get only thelane lines we are able to compute and draw the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = pipeline_grad_and_color(image)\n",
    "imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from the previous image we can get the histogram\n",
    "histogram = np.sum(img[img.shape[0]//2:,:], axis=0)\n",
    "plt.plot(histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram plot we can see the presence of two peaks. One much stronger from the other. From this two peak we can determine the lane lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_to_binary(image, thresh=(0, 255)):\n",
    "    \"\"\"\n",
    "    transform the image to binary output\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    binary_output = np.zeros_like(gray)\n",
    "    binary_output[(gray > thresh[0]) & (gray <= thresh[1])] = 1\n",
    "    # Return the result\n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_lines(image):\n",
    "    binary_warped = image_to_binary(image)\n",
    "    # Assuming you have created a warped binary image called \"binary_warped\"\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(binary_warped[binary_warped.shape[0]//2:,:], axis=0)\n",
    "    # Create an output image to draw on and  visualize the result\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]//2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 9\n",
    "    # Set height of windows\n",
    "    window_height = np.int(binary_warped.shape[0]/nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Current positions to be updated for each window\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "        # Draw the windows on the visualization image\n",
    "        cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),(0,255,0), 2) \n",
    "        cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),(0,255,0), 2) \n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds] \n",
    "\n",
    "    # Fit a second order polynomial to each\n",
    "    if leftx.size>0:\n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    else:\n",
    "        left_fit=None\n",
    "    if rightx.size>0:\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "    else:\n",
    "        right_fit = None\n",
    "    return out_img, left_fit, right_fit, nonzerox, nonzeroy,left_lane_inds,right_lane_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_lines():\n",
    "    \"\"\"\n",
    "    plot the lines in the image\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=len(test_images), ncols=2, figsize=(100, 150))\n",
    "    fig.tight_layout()\n",
    "    for i,image in enumerate(test_images):\n",
    "        ax1 = axes.flat[2*i]\n",
    "        ax2 = axes.flat[2*i+1]\n",
    "        ax1.imshow(image)\n",
    "        ax1.set_title('Original Image', fontsize=50)\n",
    "        img = pipeline_grad_and_color(image)\n",
    "        out_img, left_fit, right_fit, nonzerox,nonzeroy,left_lane_inds,right_lane_inds = find_lines(img)\n",
    "        ploty = np.linspace(0, image.shape[0]-1, image.shape[0] )\n",
    "        left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "        right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "\n",
    "        out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "        out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "    \n",
    "        # Plot the result\n",
    "        ax2.imshow(out_img)\n",
    "        ax2.set_title('Box lines', fontsize=50)\n",
    "        ax2.plot(left_fitx, ploty, color='yellow')\n",
    "        ax2.plot(right_fitx, ploty, color='yellow')\n",
    "        ax2.set_xlim(0, 1280)\n",
    "        ax2.set_ylim(720, 0)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the images and the computed lines\n",
    "#when we inspect visualy the results we can see that is very easy to lose the yellow lines when the\n",
    "#background is almost white. The second image serves as an example.\n",
    "visualize_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#if we don't find the lines we use the previous line founded\n",
    "previous_left_fit = None\n",
    "previous_right_fit = None\n",
    "\n",
    "def pipeline_lanes(image):\n",
    "    \"\"\"\n",
    "    process the image through the pipeline. \n",
    "    This pipeline includes the steps:\n",
    "    - undistord image\n",
    "    - warp image\n",
    "    - color and gradient filter\n",
    "    - find lines and plot them.\n",
    "    - compute curvature and car position\n",
    "    \"\"\"\n",
    "    global previous_left_fit, previous_right_fit\n",
    "    image = undistord_image(image)\n",
    "    warped, M, Minv = warp_image( image )\n",
    "    warped = image_to_binary(warped)\n",
    "    img = pipeline_grad_and_color(undist)\n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(warped).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "    out_img, left_fit, right_fit, nonzerox,nonzeroy,left_lane_inds,right_lane_inds = find_lines(img)\n",
    "    ploty = np.linspace(0, warped.shape[0]-1, warped.shape[0] )\n",
    "    if left_fit is not None:\n",
    "        left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "        pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "        previous_left_fit = left_fit\n",
    "    else:\n",
    "        if previous_left_fit is not None:\n",
    "            left_fitx = previous_left_fit[0]*ploty**2 + previous_left_fit[1]*ploty + previous_left_fit[2]\n",
    "            pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "        else:\n",
    "            pts_left = None\n",
    "    if right_fit is not None:\n",
    "        right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "        pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "        previous_right_fit = right_fit\n",
    "    else:\n",
    "        if previous_right_fit is not None:\n",
    "            right_fitx = previous_right_fit[0]*ploty**2 + previous_right_fit[1]*ploty + previous_right_fit[2]\n",
    "            pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "        else:\n",
    "            pts_right = None\n",
    "    \n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    if pts_left is not None and pts_right is not None:\n",
    "        pts = np.hstack((pts_left, pts_right))\n",
    "    elif pts_left is not None:\n",
    "        pts = np.hstack((pts_left,))\n",
    "    elif pts_right is not None:\n",
    "        pts = np.hstack((pts_right,))\n",
    "    else:\n",
    "        pts = None\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    if pts is not None:\n",
    "        cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (image.shape[1], image.shape[0])) \n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(undist, 1, newwarp, 0.3, 0)  \n",
    "    \n",
    "    try:\n",
    "        leftx = nonzerox[left_lane_inds]\n",
    "        lefty = nonzeroy[left_lane_inds] \n",
    "        rightx = nonzerox[right_lane_inds]\n",
    "        righty = nonzeroy[right_lane_inds]\n",
    "        y_eval = np.max(ploty)\n",
    "        # Define conversions in x and y from pixels space to meters\n",
    "        ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "        xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "        # Fit new polynomials to x,y in world space\n",
    "        left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "        right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "        # Calculate the new radii of curvature\n",
    "        left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "        right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "\n",
    "        curvature = (left_curverad + right_curverad) / 2\n",
    "        cv2.putText(result, \"Curvature: {0:.2f}m\".format(curvature), (30,60), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), thickness=10)\n",
    "    \n",
    "        bottom_leftx = left_fitx[-1]\n",
    "        bottom_rightx = right_fitx[-1]\n",
    "    \n",
    "        lane_center = (bottom_leftx + bottom_rightx) / 2\n",
    "    \n",
    "        car_center = 1280 / 2\n",
    "    \n",
    "        difference = lane_center - car_center\n",
    "    \n",
    "        difference_meters = difference * xm_per_pix\n",
    "    \n",
    "        cv2.putText(result, \"{0:.2f}m left of center\".format(difference_meters), (30,110), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), thickness=10)\n",
    "            \n",
    "    except:\n",
    "        #if we cannot compute the lines because we loose the lines show normal image\n",
    "        pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_lanes():\n",
    "    fig, axes = plt.subplots(nrows=len(test_images), ncols=4, figsize=(100, 150))\n",
    "    for i,image in enumerate(test_images):\n",
    "        axes.flat[4*i].imshow(image)\n",
    "        axes.flat[4*i+1].imshow(warp_image(image)[0])\n",
    "        axes.flat[4*i+2].imshow( pipeline_grad_and_color(image), cmap='gray')\n",
    "        axes.flat[4*i+3].imshow(pipeline_lanes(image))\n",
    "#show the test images through the pipeline so we can inspect visually the results.\n",
    "show_lanes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "previous_left_fit = previous_right_fit = None\n",
    "\n",
    "#show the project video through the final pipeline\n",
    "pipelined_video(pipeline_lanes, 'lanes_project_video.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "In good conditions is easy to find the lane lines. When the background color changes and there is shadows it's more dificult. This program doesn't take in accound weather conditions or daytime.\n",
    "Also when computing lines is more easy to have big differences with the ground truth with the fartest points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
